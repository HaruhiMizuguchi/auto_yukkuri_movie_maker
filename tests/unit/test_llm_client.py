"""
Gemini LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®çµ±åˆãƒ†ã‚¹ãƒˆ - å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ
"""

import pytest
import os
import sys
import json
import asyncio
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any

from src.api.llm_client import (
    GeminiLLMClient,
    GeminiRequest,
    GeminiResponse,
    GeminiAPIError,
    ContentBlockedError,
    ModelType,
    OutputFormat
)


# APIã‚­ãƒ¼èª­ã¿è¾¼ã¿
load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")


class TestGeminiRequest:
    """GeminiRequest ã®ãƒ†ã‚¹ãƒˆ"""
    
    def test_default_values(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãƒ†ã‚¹ãƒˆ"""
        request = GeminiRequest(prompt="Test prompt")
        
        assert request.prompt == "Test prompt"
        assert request.model == ModelType.GEMINI_2_0_FLASH
        assert request.max_output_tokens == 1024
        assert request.temperature == 0.7
        assert request.top_p == 0.9
        assert request.top_k == 40
        assert request.output_format == OutputFormat.TEXT
        assert request.structured_schema is None
        assert request.system_instruction is None
        assert request.safety_settings is None
    
    def test_custom_values(self):
        """ã‚«ã‚¹ã‚¿ãƒ å€¤è¨­å®šãƒ†ã‚¹ãƒˆ"""
        schema = {"type": "object", "properties": {"name": {"type": "string"}}}
        safety_settings = [{"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}]
        
        request = GeminiRequest(
            prompt="Custom prompt",
            model=ModelType.GEMINI_1_5_PRO,
            max_output_tokens=2048,
            temperature=0.5,
            top_p=0.8,
            top_k=20,
            output_format=OutputFormat.STRUCTURED,
            structured_schema=schema,
            system_instruction="You are a helpful assistant",
            safety_settings=safety_settings
        )
        
        assert request.prompt == "Custom prompt"
        assert request.model == ModelType.GEMINI_1_5_PRO
        assert request.max_output_tokens == 2048
        assert request.temperature == 0.5
        assert request.top_p == 0.8
        assert request.top_k == 20
        assert request.output_format == OutputFormat.STRUCTURED
        assert request.structured_schema == schema
        assert request.system_instruction == "You are a helpful assistant"
        assert request.safety_settings == safety_settings


class TestGeminiResponse:
    """GeminiResponse ã®ãƒ†ã‚¹ãƒˆ"""
    
    def test_normal_response(self):
        """é€šå¸¸ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        response = GeminiResponse(
            text="Generated text",
            model="gemini-2.0-flash",
            finish_reason="STOP",
            safety_ratings=[],
            usage_metadata={"candidatesTokenCount": 50, "promptTokenCount": 10},
            candidates=[]
        )
        
        assert response.text == "Generated text"
        assert response.model == "gemini-2.0-flash"
        assert response.finish_reason == "STOP"
        assert response.is_blocked is False
        assert response.token_count == 50
        assert response.prompt_token_count == 10
    
    def test_blocked_response(self):
        """ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        response = GeminiResponse(
            text="",
            model="gemini-2.0-flash",
            finish_reason="SAFETY",
            safety_ratings=[{"category": "HARM_CATEGORY_HARASSMENT", "probability": "HIGH"}],
            usage_metadata={},
            candidates=[]
        )
        
        assert response.is_blocked is True
        assert response.finish_reason == "SAFETY"
        assert response.token_count == 0


class TestGeminiAPIError:
    """GeminiAPIError ã®ãƒ†ã‚¹ãƒˆ"""
    
    def test_basic_error(self):
        """åŸºæœ¬ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        error = GeminiAPIError("Test error", api_error_code="400", model="gemini-2.0-flash")
        
        assert error.message == "Test error"
        assert error.api_error_code == "400"
        assert error.model == "gemini-2.0-flash"
        assert error.context["api_error_code"] == "400"
        assert error.context["model"] == "gemini-2.0-flash"
    
    def test_content_blocked_error(self):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ–ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        safety_ratings = [{"category": "HARM_CATEGORY_HARASSMENT", "probability": "HIGH"}]
        error = ContentBlockedError("Content blocked", safety_ratings)
        
        assert error.message == "Content blocked"
        assert error.error_code == "CONTENT_BLOCKED"
        assert error.retry_recommended is False
        assert error.safety_ratings == safety_ratings


@pytest.mark.skipif(not API_KEY, reason="GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
class TestGeminiLLMClientIntegration:
    """GeminiLLMClient ã®çµ±åˆãƒ†ã‚¹ãƒˆ - å®Ÿéš›ã®APIå‘¼ã³å‡ºã—"""
    
    def test_initialization(self):
        """åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        client = GeminiLLMClient(api_key=API_KEY)
        
        # APIã‚­ãƒ¼ã®è¨­å®šç¢ºèª
        assert client.api_key == API_KEY
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ç¢ºèª
        assert client._client is not None
        assert hasattr(client._client, '_api_client')
        assert hasattr(client._client._api_client, '_httpx_client')
    
    @pytest.mark.asyncio
    async def test_generate_text_simple(self):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        async with GeminiLLMClient(api_key=API_KEY) as client:
            request = GeminiRequest(prompt="ã“ã‚“ã«ã¡ã¯ã¨æ—¥æœ¬èªã§æŒ¨æ‹¶ã—ã¦ãã ã•ã„ã€‚")
            response = await client.generate_text(request)
            
            # åŸºæœ¬çš„ãªå¿œç­”ãƒã‚§ãƒƒã‚¯
            assert isinstance(response, GeminiResponse)
            assert response.text is not None
            assert len(response.text.strip()) > 0
            assert response.model == "gemini-2.0-flash"
            assert response.finish_reason == "STOP"
            assert response.is_blocked is False
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ãƒã‚§ãƒƒã‚¯
            assert response.token_count > 0
            assert response.prompt_token_count > 0
            
            print(f"âœ… ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ: {response.text[:100]}...")
            print(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {response.token_count} (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {response.prompt_token_count})")
    
    @pytest.mark.asyncio
    async def test_generate_text_with_custom_settings(self):
        """ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        async with GeminiLLMClient(api_key=API_KEY) as client:
            request = GeminiRequest(
                prompt="å¤ã®æ±äº¬ã‚’è© ã‚€ä¿³å¥ã‚’ä¸€å¥ã€å­£èªã‚’å…¥ã‚Œã¦ä½œæˆã—ã¦ãã ã•ã„ã€‚",
                temperature=0.9,
                max_output_tokens=100
            )
            response = await client.generate_text(request)
            
            assert isinstance(response, GeminiResponse)
            assert response.text is not None
            assert len(response.text.strip()) > 0
            assert response.finish_reason == "STOP"
            
            print(f"ğŸŒ¸ ä¿³å¥: {response.text.strip()}")
    
    @pytest.mark.asyncio
    async def test_generate_json_output(self):
        """JSONå½¢å¼å‡ºåŠ›ãƒ†ã‚¹ãƒˆ"""
        async with GeminiLLMClient(api_key=API_KEY) as client:
            request = GeminiRequest(
                prompt="ä»¥ä¸‹ã®æƒ…å ±ã‚’JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„: åå‰=å¤ªéƒã€å¹´é½¢=25ã€è·æ¥­=ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
                output_format=OutputFormat.JSON,
                max_output_tokens=200
            )
            response = await client.generate_text(request)
            
            assert isinstance(response, GeminiResponse)
            assert response.text is not None
            
            # JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã§ãã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            try:
                parsed_json = json.loads(response.text)
                assert isinstance(parsed_json, dict)
                print(f"ğŸ”§ JSONå‡ºåŠ›: {parsed_json}")
            except json.JSONDecodeError:
                print(f"âš ï¸ JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸãŒã€å‡ºåŠ›ã¯ç”Ÿæˆã•ã‚Œã¾ã—ãŸ: {response.text}")
    
    @pytest.mark.asyncio
    async def test_generate_structured_output(self):
        """æ§‹é€ åŒ–å‡ºåŠ›ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        async with GeminiLLMClient(api_key=API_KEY) as client:
            schema = {
                "type": "object",
                "properties": {
                    "title": {"type": "string"},
                    "description": {"type": "string"},
                    "difficulty": {"type": "string", "enum": ["easy", "medium", "hard"]}
                },
                "required": ["title", "description", "difficulty"]
            }
            
            result = await client.generate_structured_output(
                prompt="ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å­¦ç¿’ã®ãŸã‚ã®èª²é¡Œã‚’ä¸€ã¤è€ƒãˆã¦ãã ã•ã„",
                schema=schema
            )
            
            assert isinstance(result, dict)
            assert "title" in result
            assert "description" in result
            assert "difficulty" in result
            assert result["difficulty"] in ["easy", "medium", "hard"]
            
            print(f"ğŸ“‹ æ§‹é€ åŒ–å‡ºåŠ›: {result}")
    
    @pytest.mark.asyncio
    async def test_yukkuri_video_script_generation(self):
        """ã‚†ã£ãã‚Šå‹•ç”»ã®å°æœ¬ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        async with GeminiLLMClient(api_key=API_KEY) as client:
            request = GeminiRequest(
                prompt="""
                ã‚†ã£ãã‚Šå‹•ç”»ã®å°æœ¬ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
                ãƒ†ãƒ¼ãƒ: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…å‘ã‘ã®Pythonå…¥é–€
                é•·ã•: 3åˆ†ç¨‹åº¦
                
                ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:
                - ã‚¿ã‚¤ãƒˆãƒ«
                - å°å…¥éƒ¨åˆ†ï¼ˆ30ç§’ï¼‰
                - æœ¬ç·¨ï¼ˆ2åˆ†ï¼‰
                - ã¾ã¨ã‚ï¼ˆ30ç§’ï¼‰
                """,
                max_output_tokens=1000,
                temperature=0.8
            )
            response = await client.generate_text(request)
            
            assert isinstance(response, GeminiResponse)
            assert response.text is not None
            assert len(response.text.strip()) > 100  # ã‚ã‚‹ç¨‹åº¦ã®é•·ã•ãŒã‚ã‚‹ã“ã¨
            assert "ã‚¿ã‚¤ãƒˆãƒ«" in response.text or "å°å…¥" in response.text
            
            print(f"ğŸ¬ ã‚†ã£ãã‚Šå‹•ç”»å°æœ¬:\n{response.text[:500]}...")
    
    @pytest.mark.asyncio
    async def test_multiple_models(self):
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆ"""
        models_to_test = [
            ModelType.GEMINI_2_0_FLASH,
            ModelType.GEMINI_1_5_FLASH
        ]
        
        async with GeminiLLMClient(api_key=API_KEY) as client:
            for model in models_to_test:
                request = GeminiRequest(
                    prompt="ã€ŒHello Worldã€ã¨è‹±èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚",
                    model=model,
                    max_output_tokens=50
                )
                
                try:
                    response = await client.generate_text(request)
                    assert isinstance(response, GeminiResponse)
                    assert response.text is not None
                    assert len(response.text.strip()) > 0
                    
                    print(f"âœ… {model.value}: {response.text.strip()}")
                except Exception as e:
                    print(f"âŒ {model.value}: ã‚¨ãƒ©ãƒ¼ - {e}")
    
    @pytest.mark.asyncio
    async def test_count_tokens(self):
        """ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚«ã‚¦ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ"""
        async with GeminiLLMClient(api_key=API_KEY) as client:
            text = "ã“ã‚Œã¯ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ãŸã‚ã®ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚"
            token_count = await client.count_tokens(text)
            
            # æ¦‚ç®—å€¤ã®ç¢ºèª
            assert token_count > 0
            assert isinstance(token_count, int)
            
            print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆ: '{text}'")
            print(f"ğŸ“ ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_count}")
    
    @pytest.mark.asyncio
    async def test_error_handling(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        async with GeminiLLMClient(api_key=API_KEY) as client:
            # æ¥µç«¯ã«é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚¨ãƒ©ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ
            very_long_prompt = "ãƒ†ã‚¹ãƒˆ " * 10000
            request = GeminiRequest(
                prompt=very_long_prompt,
                max_output_tokens=1
            )
            
            try:
                response = await client.generate_text(request)
                print(f"âš ï¸ é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚‚æˆåŠŸ: {len(response.text)}")
            except GeminiAPIError as e:
                print(f"âœ… æœŸå¾…é€šã‚Šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e.message}")
                assert isinstance(e, GeminiAPIError)
    
    @pytest.mark.asyncio
    async def test_context_manager(self):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        async with GeminiLLMClient(api_key=API_KEY) as client:
            assert isinstance(client, GeminiLLMClient)
            
            # ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            request = GeminiRequest(prompt="ãƒ†ã‚¹ãƒˆã§ã™ã€‚")
            response = await client.generate_text(request)
            assert response.text is not None
            
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ‚äº†å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç¢ºèª
        print("âœ… ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")


class TestPerformanceIntegration:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    @pytest.mark.skipif(not API_KEY, reason="GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    @pytest.mark.asyncio
    async def test_concurrent_requests(self):
        """ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ"""
        async with GeminiLLMClient(api_key=API_KEY) as client:
            # è¤‡æ•°ã®ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            prompts = [
                "1ã‹ã‚‰5ã¾ã§æ•°ãˆã¦ãã ã•ã„ã€‚",
                "è‰²ã®åå‰ã‚’3ã¤æ•™ãˆã¦ãã ã•ã„ã€‚",
                "ä»Šæ—¥ã®å¤©æ°—ã«ã¤ã„ã¦ä¸€è¨€ãŠé¡˜ã„ã—ã¾ã™ã€‚"
            ]
            
            import time
            start_time = time.time()
            
            # ä¸¦è¡Œå®Ÿè¡Œ
            tasks = []
            for prompt in prompts:
                request = GeminiRequest(prompt=prompt, max_output_tokens=100)
                task = client.generate_text(request)
                tasks.append(task)
            
            responses = await asyncio.gather(*tasks)
            
            end_time = time.time()
            elapsed = end_time - start_time
            
            # çµæœæ¤œè¨¼
            assert len(responses) == len(prompts)
            for response in responses:
                assert isinstance(response, GeminiResponse)
                assert response.text is not None
                assert len(response.text.strip()) > 0
            
            print(f"âš¡ ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Œäº†: {len(prompts)}ä»¶ã‚’{elapsed:.2f}ç§’ã§å‡¦ç†")
            for i, response in enumerate(responses):
                print(f"  {i+1}. {response.text.strip()[:50]}...")


if __name__ == "__main__":
    if not API_KEY:
        print("âŒ GEMINI_API_KEY ã¾ãŸã¯ GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   .env ãƒ•ã‚¡ã‚¤ãƒ«ã« API ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        sys.exit(1)
    
    print("ğŸ§ª Gemini LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆçµ±åˆãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™...")
    pytest.main([__file__, "-v", "-s"]) 